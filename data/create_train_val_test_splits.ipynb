{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataloader import load_patient_task_data_from_txt, clean_and_verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine valid columns\n",
    "\n",
    "We want to eliminate features that are not present for all samples, as we won't be able to always feed them to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "total_cols = ['RTA', 'LTA', 'IO', 'ECG', 'RGS', 'acc_x_left_shank', 'acc_y_left_shank', 'acc_z_left_shank', 'gyro_x_left_shank', 'gyro_y_left_shank', 'gyro_z_left_shank', 'NC_invalid_0', 'acc_x_right_shank', 'acc_y_right_shank', 'acc_z_right_shank', 'gyro_x_right_shank', 'gyro_y_right_shank', 'gyro_z_right_shank', 'NC_invalid_1', 'acc_x_waist', 'acc_y_waist', 'acc_z_waist', 'gyro_x_waist', 'gyro_y_waist', 'gyro_z_waist', 'NC_invalid_2', 'acc_x_arm', 'acc_y_arm', 'acc_z_arm', 'gyro_x_arm', 'gyro_y_arm', 'gyro_z_arm', 'SC']\n",
    "\n",
    "# Merging left_shank and right_shank to shank\n",
    "for i, col in enumerate(total_cols):\n",
    "    if 'left' in col:\n",
    "        total_cols[i] = ''.join(col.split('_left'))\n",
    "    if 'right' in col:\n",
    "        total_cols[i] = ''.join(col.split('_right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for patient_id=001, task=5\n",
      "No data found for patient_id=001, task=6\n",
      "No data found for patient_id=002, task=5\n",
      "No data found for patient_id=002, task=6\n",
      "No data found for patient_id=003, task=5\n",
      "No data found for patient_id=003, task=6\n",
      "No data found for patient_id=004, task=6\n",
      "No data found for patient_id=005, task=5\n",
      "No data found for patient_id=005, task=6\n",
      "No data found for patient_id=006, task=5\n",
      "No data found for patient_id=006, task=6\n",
      "No data found for patient_id=007, task=5\n",
      "No data found for patient_id=007, task=6\n",
      "No data found for patient_id=008-1, task=6\n",
      "No data found for patient_id=008-2, task=5\n",
      "No data found for patient_id=008-2, task=6\n",
      "No data found for patient_id=010, task=5\n",
      "No data found for patient_id=010, task=6\n",
      "No data found for patient_id=011, task=5\n",
      "No data found for patient_id=011, task=6\n",
      "No data found for patient_id=012, task=5\n",
      "No data found for patient_id=012, task=6\n",
      "\n",
      "Cannot use the following features: ['gyro_z_arm', 'NC_invalid_2', 'gyro_y_waist', 'acc_z_waist', 'acc_x_waist', 'acc_y_waist', 'acc_y_arm', 'gyro_y_arm', 'SC', 'gyro_z_waist', 'acc_z_arm', 'gyro_x_waist', 'NC_invalid_1', 'NC_invalid_0', 'gyro_x_arm', 'acc_x_arm']\n"
     ]
    }
   ],
   "source": [
    "# Get list of unusable features\n",
    "unusable = []\n",
    "for patient_id in ['001', '002', '003', '004', '005', '006', '007', '008-1', '008-2', '009', '010', '011', '012']:\n",
    "    for task_num in [i for i in range(1, 7)]:\n",
    "        patient_x_task_y_data = load_patient_task_data_from_txt(patient_id, task_num)\n",
    "        patient_x_task_y_data = clean_and_verify(patient_x_task_y_data)\n",
    "        \n",
    "        if not patient_x_task_y_data.empty:\n",
    "            cols = patient_x_task_y_data.columns.values.tolist()\n",
    "            unusable += [col for col in total_cols if col not in cols]\n",
    "        else:\n",
    "            print(f'No data found for patient_id={patient_id}, task={task_num}')\n",
    "                  \n",
    "unusable = list(set(unusable))\n",
    "print(f'\\nCannot use the following features: {unusable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use the following features: ['RTA', 'LTA', 'IO', 'ECG', 'RGS', 'acc_x_shank', 'acc_y_shank', 'acc_z_shank', 'gyro_x_shank', 'gyro_y_shank', 'gyro_z_shank', 'acc_x_shank', 'acc_y_shank', 'acc_z_shank', 'gyro_x_shank', 'gyro_y_shank', 'gyro_z_shank', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Get list of usable features\n",
    "usable = [col for col in total_cols if col not in unusable] + ['label']\n",
    "print(f'We can use the following features: {usable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amalgamate data from different patients, tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting for patient 001, task 1...\n",
      "Index(['RTA', 'LTA', 'IO', 'ECG', 'RGS', 'acc_x_shank', 'acc_y_shank',\n",
      "       'acc_z_shank', 'gyro_x_shank', 'gyro_y_shank', 'gyro_z_shank',\n",
      "       'acc_x_shank', 'acc_y_shank', 'acc_z_shank', 'gyro_x_shank',\n",
      "       'gyro_y_shank', 'gyro_z_shank', 'label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 302.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting for patient 001, task 2...\n",
      "Index(['RTA', 'LTA', 'IO', 'ECG', 'RGS', 'acc_x_shank', 'acc_y_shank',\n",
      "       'acc_z_shank', 'gyro_x_shank', 'gyro_y_shank', 'gyro_z_shank',\n",
      "       'acc_x_shank', 'acc_y_shank', 'acc_z_shank', 'gyro_x_shank',\n",
      "       'gyro_y_shank', 'gyro_z_shank', 'label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [00:00<00:00, 392.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting for patient 001, task 3...\n",
      "\n",
      "Collecting for patient 001, task 4...\n",
      "\n",
      "Collecting for patient 001, task 5...\n",
      "\n",
      "Collecting for patient 001, task 6...\n",
      "\n",
      "Collecting for patient 002, task 1...\n",
      "\n",
      "Collecting for patient 002, task 2...\n",
      "\n",
      "Collecting for patient 002, task 3...\n",
      "\n",
      "Collecting for patient 002, task 4...\n",
      "Index(['RTA', 'LTA', 'IO', 'ECG', 'RGS', 'acc_x_shank', 'acc_y_shank',\n",
      "       'acc_z_shank', 'gyro_x_shank', 'gyro_y_shank', 'gyro_z_shank',\n",
      "       'acc_x_shank', 'acc_y_shank', 'acc_z_shank', 'gyro_x_shank',\n",
      "       'gyro_y_shank', 'gyro_z_shank', 'label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 4744.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting for patient 002, task 5...\n",
      "\n",
      "Collecting for patient 002, task 6...\n",
      "\n",
      "Collecting for patient 003, task 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_num \u001b[38;5;129;01min\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCollecting for patient \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     patient_x_task_y_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_patient_task_data_from_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     patient_x_task_y_data \u001b[38;5;241m=\u001b[39m clean_and_verify(patient_x_task_y_data)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m patient_x_task_y_data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/Desktop/SYDE 599/Project/data/dataloader.py:67\u001b[0m, in \u001b[0;36mload_patient_task_data_from_txt\u001b[0;34m(patient_id, task_num)\u001b[0m\n\u001b[1;32m     64\u001b[0m     patient_folder \u001b[38;5;241m=\u001b[39m patient_id\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/task_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpatient_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/task_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1289\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m   1274\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1275\u001b[0m     dialect,\n\u001b[1;32m   1276\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1286\u001b[0m )\n\u001b[1;32m   1287\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/599Project/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "for patient_id in ['001', '002', '003', '004', '005', '006', '007', '008-1', '008-2', '009', '010', '011', '012']:\n",
    "    patient_data = None  # filled with [n_samples, window_size, n_features]\n",
    "    for task_num in [i for i in range(1, 7)]:\n",
    "        print(f'\\nCollecting for patient {patient_id}, task {task_num}...')\n",
    "        \n",
    "        patient_x_task_y_data = load_patient_task_data_from_txt(patient_id, task_num)\n",
    "        patient_x_task_y_data = clean_and_verify(patient_x_task_y_data)\n",
    "        \n",
    "        if 'label' not in patient_x_task_y_data.columns:\n",
    "            continue\n",
    "        \n",
    "        # Remove unusable columns\n",
    "        patient_x_task_y_data = patient_x_task_y_data[usable]\n",
    "        \n",
    "        # Break into windows\n",
    "        for i in tqdm(range(len(patient_x_task_y_data) // window_size)):\n",
    "            window = patient_x_task_y_data.loc[i * window_size: i * window_size + window_size - 1].values\n",
    "            window = np.expand_dims(window, axis=0)\n",
    "            \n",
    "            if patient_data is None:\n",
    "                patient_data = window\n",
    "            else:\n",
    "                patient_data = np.concatenate([patient_data, window], axis=0)\n",
    "                \n",
    "    np.save(f'{patient_id}.npy', patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = None\n",
    "for patient_id in ['001', '002', '003', '004', '005', '006', '007', '008-1', '008-2', '009', '010', '011', '012']:\n",
    "    patient_data = np.load(f'{patient_id}.npy', allow_pickle=True)\n",
    "    \n",
    "    if all_data is None:\n",
    "        all_data = patient_data\n",
    "    else:\n",
    "        if len(patient_data.shape) == 3:\n",
    "            all_data = np.concatenate([all_data, patient_data], axis=0)\n",
    "            \n",
    "    os.remove(f'{patient_id}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_data[:, :, 0:-1]\n",
    "y = all_data[:, :, -1]\n",
    "\n",
    "col_headers = np.array(usable)\n",
    "\n",
    "x_train, x_val_test, y_train, y_val_test = train_test_split(x, y, train_size=0.7, random_state=0)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, train_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('training_data'):\n",
    "    os.mkdir('training_data')\n",
    "\n",
    "np.save('training_data/headers.npy', col_headers)\n",
    "    \n",
    "np.save('training_data/x_train.npy', x_train)\n",
    "np.save('training_data/x_val.npy', x_val)\n",
    "np.save('training_data/x_test.npy', x_test)\n",
    "\n",
    "np.save('training_data/y_train.npy', y_train)\n",
    "np.save('training_data/y_val.npy', y_val)\n",
    "np.save('training_data/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
